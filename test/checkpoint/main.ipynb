{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotAbsPathException(Exception):\n",
    "    def __init__(self,path, *args: object) -> None:\n",
    "        super().__init__(*args)\n",
    "        self.path = path\n",
    "    def __str__(self):\n",
    "        return f\"{self.path} is not an abstract path\"\n",
    "\n",
    "class CheckPoint:\n",
    "    def __init__(self,path,file_name) -> None:\n",
    "        \"\"\"\n",
    "            检查path是否是绝对路径\n",
    "            检查多级目录是否创建\n",
    "        \"\"\"\n",
    "        if not os.path.isabs(path):\n",
    "            raise NotAbsPathException(path)\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        self.path = path\n",
    "        self.file_name = file_name\n",
    "        self.file_path = os.path.join(self.path,self.file_name)\n",
    "\n",
    "    def save(self,data:dict):\n",
    "        \"\"\"\n",
    "            保存数据, 所有数据完全由用户提供(不同情况下, 需要保存的数据不同, 没有相同的解决方案)\n",
    "        \"\"\"\n",
    "        torch.save(data,self.file_path)\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "            加载\n",
    "        \"\"\"\n",
    "        return torch.load(self.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_2NN(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # 输入：784\n",
    "        # 隐藏层 1：784*200，200\n",
    "        # 隐藏层 2：200*200，200\n",
    "        # 输出：200*10,10\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.fc_1 = torch.nn.Linear(784, 200)\n",
    "        self.fc_2 = torch.nn.Linear(200, 200)\n",
    "        self.fc_3 = torch.nn.Linear(200, 10)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def init_params(self, seed):\n",
    "        set_seed(seed)\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                # 参数初始化方法一般与激活函数有关\n",
    "                # Relu-kaming\n",
    "                # sigmoid-xavier\n",
    "                torch.nn.init.kaiming_normal_(layer.weight.data)\n",
    "                torch.nn.init.zeros_(layer.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.nn.Dropout(0.5)(x)  # 过拟合\n",
    "        x = self.fc_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.nn.Dropout(0.5)(x)  # 过拟合\n",
    "        x = self.fc_3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_flag = True # 是否从目录重新加载\n",
    "log_dir = '/home/whr-pc-ubuntu/code/test/checkpoint/log/1111'\n",
    "file_name = 'models.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MNIST_2NN()\n",
    "net.init_params(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = CheckPoint(log_dir,file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.save({\n",
    "    \"model\":net.state_dict(),\n",
    "    \"step\":0\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': OrderedDict([('fc_1.weight',\n",
       "               tensor([[-0.0569, -0.0582, -0.0127,  ..., -0.0799, -0.0297, -0.0058],\n",
       "                       [ 0.0354, -0.0281, -0.0193,  ...,  0.0201,  0.0130,  0.0101],\n",
       "                       [-0.0080,  0.0373, -0.0127,  ...,  0.0465, -0.0056, -0.0946],\n",
       "                       ...,\n",
       "                       [ 0.0469, -0.0082,  0.0193,  ...,  0.0332, -0.0398, -0.0023],\n",
       "                       [-0.0562,  0.0057, -0.0071,  ...,  0.0176,  0.0732, -0.1302],\n",
       "                       [-0.0638,  0.1276, -0.0920,  ..., -0.0135, -0.0364, -0.1337]])),\n",
       "              ('fc_1.bias',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('fc_2.weight',\n",
       "               tensor([[-0.0722, -0.1315, -0.0226,  ..., -0.2176, -0.1793, -0.0266],\n",
       "                       [ 0.0985,  0.1414, -0.1288,  ..., -0.0294,  0.0418, -0.0042],\n",
       "                       [ 0.1272,  0.0326, -0.0473,  ...,  0.2009, -0.1764, -0.0042],\n",
       "                       ...,\n",
       "                       [ 0.0343, -0.2126,  0.0922,  ..., -0.0645, -0.0035, -0.0794],\n",
       "                       [ 0.1270, -0.0605, -0.0718,  ..., -0.0172, -0.0517,  0.0251],\n",
       "                       [-0.0205, -0.0162, -0.0770,  ..., -0.0911, -0.0793, -0.0390]])),\n",
       "              ('fc_2.bias',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('fc_3.weight',\n",
       "               tensor([[ 0.1754, -0.1504, -0.0906,  ..., -0.1142, -0.0552, -0.1355],\n",
       "                       [-0.0829,  0.1394, -0.1570,  ...,  0.1232,  0.2199,  0.0552],\n",
       "                       [ 0.0927,  0.0787, -0.0635,  ..., -0.0886, -0.1091,  0.1068],\n",
       "                       ...,\n",
       "                       [-0.0377,  0.2080, -0.0109,  ...,  0.0010, -0.1019, -0.1310],\n",
       "                       [-0.2304,  0.1669,  0.0563,  ..., -0.0197,  0.0079,  0.1777],\n",
       "                       [-0.1246, -0.0543,  0.0702,  ...,  0.0316,  0.0400, -0.0978]])),\n",
       "              ('fc_3.bias',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))]),\n",
       " 'step': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_flag = False # 是否从目录重新加载\n",
    "log_dir = '/home/whr-pc-ubuntu/code/test/checkpoint/log/1111'\n",
    "file_name = 'models.pth'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test\n",
    "\n",
    "先训练一段时间, 将结果记录在tensor board中\n",
    "\n",
    "然后将reload_flag修改为True, 观察tensor board的曲线是否是连贯的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_str():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "\n",
    "def get_writer(*tags):\n",
    "    path = 'logs'\n",
    "    for tag in tags:\n",
    "        path = join(path, tag)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    return writer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seed=0, batch_size=256, shuffle=True):\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081)),\n",
    "    ])\n",
    "    dataset_path = '/home/whr-pc-ubuntu/code/dataset'\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        dataset_path,\n",
    "        True,\n",
    "        transform,\n",
    "        download=True,\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        dataset_path,\n",
    "        False,\n",
    "        transform,\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    set_seed(seed)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size, shuffle)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1000\n",
    "seed = 0\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "train_dataloader,test_dataloader = load_data(seed,batch_size,False)\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/home/whr-pc-ubuntu/code/test/checkpoint/log/5555'\n",
    "file_name = 'models.pth'\n",
    "checkpoint = CheckPoint(log_dir,file_name)\n",
    "writer = get_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MNIST_2NN()\n",
    "net.init_params(seed)\n",
    "net.to(device)\n",
    "optim = torch.optim.SGD(net.parameters(),lr)    \n",
    "start = 0\n",
    "\n",
    "if load_flag:\n",
    "    data = checkpoint.load()\n",
    "    net.load_state_dict(data['model'])\n",
    "    # optim.load_state_dict(data['optim'])\n",
    "    start = data['i'] + 1\n",
    "    \n",
    "net.eval()    \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "accumulator = d2l.Accumulator(3)\n",
    "for i in range(start,epoch):\n",
    "    for x,y in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        y_hat = net(x)\n",
    "\n",
    "        loss = loss_func(y_hat,y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        accumulator.add(loss*x.shape[0],d2l.accuracy(y_hat,y),x.shape[0])\n",
    "\n",
    "    test_acc = d2l.evaluate_accuracy_gpu(net,test_dataloader,device)\n",
    "\n",
    "    writer.add_scalar(\"train loss\",accumulator[0]/accumulator[-1],i)\n",
    "    writer.add_scalar(\"train acc\",accumulator[1]/accumulator[-1],i)\n",
    "    writer.add_scalar(\"test acc\",test_acc,i)\n",
    "    \n",
    "    \n",
    "    # 间隔20次保存一次\n",
    "    if i%20 == 0:\n",
    "        checkpoint.save({\n",
    "            \"model\":net.state_dict(),\n",
    "            \"optim\":optim.state_dict(),\n",
    "            \"i\":i\n",
    "            })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_flag = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1000\n",
    "seed = 0\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "train_dataloader,test_dataloader = load_data(seed,batch_size,False)\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/home/whr-pc-ubuntu/code/test/checkpoint/log/5555'\n",
    "file_name = 'models.pth'\n",
    "checkpoint = CheckPoint(log_dir,file_name)\n",
    "writer = get_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload\n"
     ]
    }
   ],
   "source": [
    "net = MNIST_2NN()\n",
    "net.init_params(seed)\n",
    "net.to(device)\n",
    "optim = torch.optim.SGD(net.parameters(),lr)    \n",
    "start = 0\n",
    "\n",
    "if load_flag:\n",
    "    print(\"reload\")\n",
    "    data = checkpoint.load()\n",
    "    net.load_state_dict(data['model'])\n",
    "    optim.load_state_dict(data['optim'])\n",
    "    start = data['i'] + 1\n",
    "\n",
    "net.eval()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "accumulator = d2l.Accumulator(3)\n",
    "for i in range(start,epoch):\n",
    "    for x,y in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        y_hat = net(x)\n",
    "\n",
    "        loss = loss_func(y_hat,y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        accumulator.add(loss*x.shape[0],d2l.accuracy(y_hat,y),x.shape[0])\n",
    "\n",
    "    test_acc = d2l.evaluate_accuracy_gpu(net,test_dataloader,device)\n",
    "\n",
    "    writer.add_scalar(\"train loss\",accumulator[0]/accumulator[-1],i)\n",
    "    writer.add_scalar(\"train acc\",accumulator[1]/accumulator[-1],i)\n",
    "    writer.add_scalar(\"test acc\",test_acc,i)\n",
    "    \n",
    "    # 间隔20次保存一次\n",
    "    if i%20 == 0:\n",
    "        checkpoint.save({\n",
    "            \"model\":net.state_dict(),\n",
    "            \"optim\":optim.state_dict(),\n",
    "            \"i\":i\n",
    "            })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# epoch等效替代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 40\n",
    "seed = 0\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "train_dataloader,test_dataloader = load_data(seed,batch_size,False)\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/home/whr-pc-ubuntu/code/test/checkpoint/log/3333'\n",
    "file_name = 'models.pth'\n",
    "checkpoint = CheckPoint(log_dir,file_name)\n",
    "writer = get_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MNIST_2NN()\n",
    "net.init_params(seed)\n",
    "net.to(device)\n",
    "optim = torch.optim.SGD(net.parameters(),lr)    \n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "accumulator = d2l.Accumulator(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,1000):\n",
    "    for x,y in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        y_hat = net(x)\n",
    "\n",
    "        loss = loss_func(y_hat,y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        accumulator.add(loss*x.shape[0],d2l.accuracy(y_hat,y),x.shape[0])\n",
    "\n",
    "    writer.add_scalar(\"train loss\",accumulator[0]/accumulator[-1],i)\n",
    "    writer.add_scalar(\"train acc\",accumulator[1]/accumulator[-1],i)\n",
    "    \n",
    "    # 间隔20次保存一次\n",
    "    if i%20 == 0:\n",
    "        checkpoint.save({\n",
    "            \"model\":net.state_dict(),\n",
    "            \"optim\":optim.state_dict(),\n",
    "            \"i\":i\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch,2*epoch):\n",
    "    for x,y in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        y_hat = net(x)\n",
    "\n",
    "        loss = loss_func(y_hat,y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        accumulator.add(loss*x.shape[0],d2l.accuracy(y_hat,y),x.shape[0])\n",
    "\n",
    "    writer.add_scalar(\"train loss\",accumulator[0]/accumulator[-1],i)\n",
    "    writer.add_scalar(\"train acc\",accumulator[1]/accumulator[-1],i)\n",
    "    \n",
    "    # 间隔20次保存一次\n",
    "    if i%20 == 0:\n",
    "        checkpoint.save({\n",
    "            \"model\":net.state_dict(),\n",
    "            \"optim\":optim.state_dict(),\n",
    "            \"i\":i\n",
    "            })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
