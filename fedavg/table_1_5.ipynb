{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whr-pc-ubuntu/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/whr-pc-ubuntu/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchsummary\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_number = 100\n",
    "seed = 0\n",
    "B = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_IID(client_number, seed):\n",
    "    # shuffle,fix the seed\n",
    "    # 100 clients, each 100 examples\n",
    "    dataset_path = \"/home/whr-pc-ubuntu/code/dataset\"\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081)), # 归一化，有利于训练\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.MNIST(dataset_path, True, transform, download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(dataset_path, False, transform, download=True)\n",
    "\n",
    "    slice_num = int(len(train_dataset) / client_number)\n",
    "    split_list = [slice_num]*(client_number-1)\n",
    "    split_list.append(len(train_dataset)-sum(split_list))\n",
    "\n",
    "    set_seed(seed)\n",
    "    train_datasets = random_split(train_dataset, split_list)\n",
    "\n",
    "    return train_datasets, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets, test_dataset = load_data_IID(client_number, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets[0][1][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "9\n",
      "6\n",
      "8\n",
      "8\n",
      "7\n",
      "4\n",
      "8\n",
      "9\n",
      "4\n",
      "0\n",
      "6\n",
      "8\n",
      "9\n",
      "9\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "5\n",
      "0\n",
      "2\n",
      "1\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "1\n",
      "8\n",
      "7\n",
      "3\n",
      "3\n",
      "5\n",
      "1\n",
      "6\n",
      "0\n",
      "1\n",
      "4\n",
      "4\n",
      "0\n",
      "0\n",
      "7\n",
      "0\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "2\n",
      "1\n",
      "0\n",
      "7\n",
      "9\n",
      "2\n",
      "6\n",
      "8\n",
      "7\n",
      "7\n",
      "2\n",
      "8\n",
      "8\n",
      "5\n",
      "7\n",
      "1\n",
      "9\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "8\n",
      "6\n",
      "5\n",
      "1\n",
      "6\n",
      "5\n",
      "6\n",
      "8\n",
      "8\n",
      "5\n",
      "0\n",
      "2\n",
      "7\n",
      "2\n",
      "5\n",
      "8\n",
      "7\n",
      "7\n",
      "7\n",
      "2\n",
      "7\n",
      "8\n",
      "8\n",
      "7\n",
      "7\n",
      "0\n",
      "2\n",
      "9\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "6\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "8\n",
      "8\n",
      "7\n",
      "6\n",
      "2\n",
      "0\n",
      "7\n",
      "3\n",
      "9\n",
      "2\n",
      "1\n",
      "4\n",
      "7\n",
      "1\n",
      "6\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "9\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "6\n",
      "0\n",
      "7\n",
      "9\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "9\n",
      "7\n",
      "3\n",
      "5\n",
      "2\n",
      "7\n",
      "4\n",
      "7\n",
      "7\n",
      "2\n",
      "8\n",
      "1\n",
      "1\n",
      "1\n",
      "9\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "8\n",
      "9\n",
      "9\n",
      "7\n",
      "6\n",
      "3\n",
      "9\n",
      "3\n",
      "4\n",
      "0\n",
      "9\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "9\n",
      "1\n",
      "6\n",
      "4\n",
      "5\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "5\n",
      "2\n",
      "1\n",
      "2\n",
      "8\n",
      "8\n",
      "9\n",
      "6\n",
      "9\n",
      "3\n",
      "6\n",
      "3\n",
      "1\n",
      "7\n",
      "8\n",
      "7\n",
      "2\n",
      "1\n",
      "4\n",
      "6\n",
      "1\n",
      "5\n",
      "3\n",
      "1\n",
      "5\n",
      "9\n",
      "3\n",
      "6\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "3\n",
      "7\n",
      "2\n",
      "8\n",
      "1\n",
      "7\n",
      "7\n",
      "1\n",
      "7\n",
      "4\n",
      "4\n",
      "1\n",
      "4\n",
      "7\n",
      "0\n",
      "8\n",
      "7\n",
      "4\n",
      "1\n",
      "3\n",
      "4\n",
      "5\n",
      "7\n",
      "4\n",
      "4\n",
      "7\n",
      "2\n",
      "3\n",
      "2\n",
      "5\n",
      "6\n",
      "8\n",
      "3\n",
      "8\n",
      "9\n",
      "5\n",
      "7\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "8\n",
      "0\n",
      "5\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "9\n",
      "7\n",
      "5\n",
      "4\n",
      "0\n",
      "4\n",
      "8\n",
      "7\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "9\n",
      "2\n",
      "8\n",
      "1\n",
      "2\n",
      "7\n",
      "3\n",
      "5\n",
      "9\n",
      "1\n",
      "8\n",
      "2\n",
      "5\n",
      "8\n",
      "5\n",
      "0\n",
      "3\n",
      "1\n",
      "6\n",
      "0\n",
      "7\n",
      "8\n",
      "6\n",
      "4\n",
      "8\n",
      "8\n",
      "3\n",
      "6\n",
      "9\n",
      "8\n",
      "2\n",
      "0\n",
      "9\n",
      "1\n",
      "5\n",
      "9\n",
      "2\n",
      "9\n",
      "6\n",
      "1\n",
      "5\n",
      "8\n",
      "1\n",
      "7\n",
      "3\n",
      "7\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "8\n",
      "6\n",
      "8\n",
      "5\n",
      "0\n",
      "9\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "9\n",
      "3\n",
      "3\n",
      "7\n",
      "9\n",
      "2\n",
      "4\n",
      "3\n",
      "8\n",
      "8\n",
      "1\n",
      "6\n",
      "8\n",
      "6\n",
      "8\n",
      "9\n",
      "0\n",
      "3\n",
      "8\n",
      "1\n",
      "2\n",
      "2\n",
      "9\n",
      "9\n",
      "6\n",
      "6\n",
      "8\n",
      "7\n",
      "4\n",
      "4\n",
      "3\n",
      "0\n",
      "1\n",
      "9\n",
      "3\n",
      "1\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "9\n",
      "0\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "8\n",
      "9\n",
      "3\n",
      "8\n",
      "5\n",
      "5\n",
      "6\n",
      "7\n",
      "7\n",
      "1\n",
      "6\n",
      "0\n",
      "7\n",
      "0\n",
      "5\n",
      "7\n",
      "8\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "7\n",
      "9\n",
      "9\n",
      "0\n",
      "0\n",
      "7\n",
      "4\n",
      "3\n",
      "9\n",
      "7\n",
      "6\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "4\n",
      "5\n",
      "2\n",
      "6\n",
      "5\n",
      "6\n",
      "3\n",
      "3\n",
      "8\n",
      "5\n",
      "1\n",
      "7\n",
      "0\n",
      "2\n",
      "9\n",
      "1\n",
      "1\n",
      "7\n",
      "7\n",
      "0\n",
      "1\n",
      "9\n",
      "8\n",
      "7\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "1\n",
      "5\n",
      "3\n",
      "2\n",
      "5\n",
      "0\n",
      "9\n",
      "3\n",
      "4\n",
      "0\n",
      "7\n",
      "3\n",
      "0\n",
      "2\n",
      "5\n",
      "1\n",
      "3\n",
      "5\n",
      "9\n",
      "5\n",
      "6\n",
      "2\n",
      "5\n",
      "1\n",
      "7\n",
      "4\n",
      "9\n",
      "6\n",
      "9\n",
      "4\n",
      "4\n",
      "3\n",
      "7\n",
      "0\n",
      "9\n",
      "1\n",
      "2\n",
      "6\n",
      "6\n",
      "5\n",
      "0\n",
      "6\n",
      "6\n",
      "9\n",
      "4\n",
      "0\n",
      "7\n",
      "8\n",
      "1\n",
      "2\n",
      "7\n",
      "6\n",
      "7\n",
      "6\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "6\n",
      "8\n",
      "6\n",
      "6\n",
      "7\n",
      "8\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "9\n",
      "4\n",
      "2\n",
      "9\n",
      "6\n",
      "1\n",
      "8\n",
      "1\n",
      "3\n",
      "7\n",
      "4\n",
      "8\n",
      "9\n",
      "1\n",
      "1\n",
      "4\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "6\n",
      "4\n",
      "1\n",
      "1\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "8\n",
      "3\n",
      "3\n",
      "6\n",
      "7\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "5\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "9\n",
      "0\n",
      "2\n",
      "2\n",
      "5\n",
      "7\n",
      "6\n",
      "4\n",
      "8\n",
      "5\n",
      "4\n",
      "5\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in train_datasets[0]:\n",
    "    print(i[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_Non_IDD(client_number):\n",
    "    # short by digit label, ascending\n",
    "    # 200 shards, each 300 examples\n",
    "    # 100 clients, each 2 shards\n",
    "    # that is, 100 clients, each 600 examples\n",
    "    dataset_path = \"/home/whr-pc-ubuntu/code/dataset\"\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.MNIST(dataset_path, True, transform, download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(dataset_path, False, transform, download=True)\n",
    "\n",
    "    # before = [i[1]for i in train_dataset]\n",
    "    # print(before)\n",
    "    train_dataset = sorted(train_dataset, key=lambda x: x[1])\n",
    "    # after = [i[1] for i in train_dataset]\n",
    "    # print(after)\n",
    "\n",
    "    slice_num = int(len(train_dataset) / client_number)\n",
    "    train_datasets = []\n",
    "    for i in range(client_number-1):\n",
    "        train_datasets.append(train_dataset[i*slice_num:(i+1)*slice_num])\n",
    "    train_datasets.append(train_dataset[(client_number-1)*slice_num:])\n",
    "\n",
    "    return train_datasets, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets, test_dataset = load_data_Non_IDD(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets[0][1][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in train_datasets[0]:\n",
    "    print(i[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络结构\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST_2NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_2NN(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # 输入：784\n",
    "        # 隐藏层 1：784*200，200\n",
    "        # 隐藏层 2：200*200，200\n",
    "        # 输出：200*10,10\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.fc_1 = torch.nn.Linear(784, 200)\n",
    "        self.fc_2 = torch.nn.Linear(200, 200)\n",
    "        self.fc_3 = torch.nn.Linear(200, 10)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def init_params(self, seed):\n",
    "        set_seed(seed)\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                # 参数初始化方法一般与激活函数有关\n",
    "                # Relu-kaming\n",
    "                # sigmoid-xavier\n",
    "                nn.init.kaiming_normal_(layer.weight.data)\n",
    "                nn.init.zeros_(layer.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = nn.Dropout(0.5)(x)  # 过拟合\n",
    "        x = self.fc_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = nn.Dropout(0.5)(x)  # 过拟合\n",
    "        x = self.fc_3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_global_net(global_net, local_net, global_num, local_num):\n",
    "    index = 1.0 * local_num / global_num\n",
    "    optim_1 = torch.optim.SGD(global_net.parameters(), 0.1)  # whatever the lr is\n",
    "    optim_2 = torch.optim.SGD(local_net.parameters(), 0.1)  # whatever the lr is\n",
    "\n",
    "    for param_1, param_2 in zip(optim_1.param_groups[0]['params'], optim_2.param_groups[0]['params']):\n",
    "        param_1.data = param_1.data + index * param_2.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_net_zero(global_net):\n",
    "    optim = torch.optim.SGD(global_net.parameters(), 0.1)  # whatever the lr is\n",
    "\n",
    "    for param in optim.param_groups[0]['params']:\n",
    "        param.data.zero_()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算 test acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_acc(net,test_dataloader,device=torch.device(\"cpu:0\")):\n",
    "    with torch.no_grad():\n",
    "        sum = 0\n",
    "        for x,y in test_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_hat = net(x)\n",
    "            sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "    return 1.0 * sum / len(test_dataset)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0954"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MNIST_2NN()\n",
    "net.init_params(seed)\n",
    "get_test_acc(net, test_dataloader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST_2NN exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0  # to initialize the global net\n",
    "E = 1  # epoch\n",
    "client_number = 100  # client_number\n",
    "C_list = [0, 0.1, 0.2, 0.5, 1.0]  # m=max(c*client_num,1)\n",
    "test_acc_target = 0.97  # when to stop the iteration\n",
    "lr = 0.5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets, test_dataset = load_data_IID(client_number, seed)\n",
    "test_dataloader = DataLoader(test_dataset, 128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_update(global_net, train_dataloader, E, lr,device=torch.device(\"cpu:0\")):\n",
    "    \"\"\"\n",
    "        return net, loss, acc\n",
    "    \"\"\"\n",
    "    # deep copy global_net\n",
    "    local_net = deepcopy(global_net)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.SGD(local_net.parameters(), lr,weight_decay=0.01) # 过拟合\n",
    "    accumulator = d2l.Accumulator(3)\n",
    "    for e in range(E):  # epoch E\n",
    "        for x, y in train_dataloader:  # batch size B\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            y_hat = local_net(x)\n",
    "            loss = loss_function(y_hat, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            accumulator.add(loss*x.shape[0],d2l.accuracy(y_hat,y),x.shape[0])\n",
    "            \n",
    "    return local_net,accumulator[0] / accumulator[2], accumulator[1]/accumulator[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_str():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_writer(*tags):\n",
    "    path = 'logs'\n",
    "    for tag in tags:\n",
    "        path = join(path, tag)\n",
    "    log_dir = join(path, now_str())\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    return writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IID_train(C_list, E,B, lr, seed, train_dataloaders, test_dataset, test_acc_target, client_number):\n",
    "    for C in C_list:\n",
    "        writer = get_writer('MNIST_2NN', 'IID', f'B={B}', f'C={C}')\n",
    "        global_net = MNIST_2NN()\n",
    "        global_net.init_params(seed)\n",
    "        step = 0\n",
    "        test_acc = 0\n",
    "        while test_acc < test_acc_target:  # control the variable t by the acc target\n",
    "            m = max(int(C*client_number), 1)\n",
    "            client_indexs = random.sample(range(0, client_number), m)  # select m clients randomly\n",
    "\n",
    "            client_nets = []  # store net(t+1,client_index) by local update\n",
    "            train_loss = []\n",
    "            accumulater = d2l.Accumulator(3)\n",
    "            for client_index in client_indexs:\n",
    "                client_net,train_loss,train_acc = client_update(global_net, train_dataloaders[client_index], E, lr)\n",
    "                client_nets.append(client_net)\n",
    "                length = len(train_dataloaders[client_index])\n",
    "                accumulater.add(train_loss*length,train_acc*length,length)\n",
    "\n",
    "            global_net_zero(global_net)  # make global net params all zero\n",
    "            for client_index in client_indexs:  # update global net\n",
    "                update_global_net(global_net, client_nets[client_indexs.index(\n",
    "                    client_index)], 60000, len(train_datasets[client_index]))\n",
    "\n",
    "            # check whether test acc reach the target\n",
    "            test_acc = get_test_acc(global_net, test_dataset)\n",
    "            step += 1\n",
    "            \n",
    "            writer.add_scalar(\"train loss\", accumulater[0] / accumulater[2], step)\n",
    "            writer.add_scalar(\"train acc\", accumulater[1] / accumulater[2], step)\n",
    "            writer.add_scalar(\"test acc\", test_acc, step)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 10  # batch size for all clients\n",
    "# 加载数据，当B变化时，数据不同\n",
    "train_dataloaders = [DataLoader(train_dataset, len(train_dataset) if B == 'inf' else B,\n",
    "                                shuffle=False) for train_dataset in train_datasets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01 # 如果太高，容易过拟合"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in C_list:\n",
    "    writer = get_writer('MNIST_2NN', 'IID', f'B={B}', f'C={C}')\n",
    "    global_net = MNIST_2NN()\n",
    "    global_net.init_params(seed)\n",
    "    step = 0\n",
    "    test_acc = 0\n",
    "    while test_acc < test_acc_target:  # control the variable t by the acc target\n",
    "        m = max(int(C*client_number), 1)\n",
    "        client_indexs = random.sample(range(0, client_number), m)  # select m clients randomly\n",
    "\n",
    "        client_nets = []  # store net(t+1,client_index) by local update\n",
    "        accumulater = d2l.Accumulator(3)\n",
    "        for client_index in client_indexs:\n",
    "            client_net,train_loss,train_acc = client_update(global_net, train_dataloaders[client_index], E, lr)\n",
    "            client_nets.append(client_net)\n",
    "            length = len(train_dataloaders[client_index])\n",
    "            accumulater.add(train_loss*length,train_acc*length,length)\n",
    "\n",
    "        global_net_zero(global_net)  # make global net params all zero\n",
    "        n = 0 # get n. n should be the sum of examples in variable client_nets, not 60000\n",
    "        for client_index in client_indexs:\n",
    "            n += len(train_datasets[client_index])\n",
    "        for client_index in client_indexs:  # update global net\n",
    "            update_global_net(global_net, client_nets[client_indexs.index(client_index)], n, len(train_datasets[client_index]))\n",
    "\n",
    "        # check whether test acc reach the target\n",
    "        test_acc = get_test_acc(global_net, test_datloader)\n",
    "        step += 1\n",
    "        \n",
    "        writer.add_scalar(\"train loss\", accumulater[0] / accumulater[2], step)\n",
    "        writer.add_scalar(\"train acc\", accumulater[1] / accumulater[2], step)\n",
    "        writer.add_scalar(\"test acc\", test_acc, step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = d2l.try_gpu()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in C_list:\n",
    "    writer = get_writer('MNIST_2NN', 'IID', f'B={B}', f'C={C}')\n",
    "    global_net = MNIST_2NN()\n",
    "    global_net.init_params(seed)\n",
    "    global_net.to(device)\n",
    "    step = 0\n",
    "    test_acc = 0\n",
    "    while test_acc < test_acc_target:  # control the variable t by the acc target\n",
    "        m = max(int(C*client_number), 1)\n",
    "        client_indexs = random.sample(range(0, client_number), m)  # select m clients randomly\n",
    "\n",
    "        client_nets = []  # store net(t+1,client_index) by local update\n",
    "        accumulater = d2l.Accumulator(3)\n",
    "        for client_index in client_indexs:\n",
    "            client_net,train_loss,train_acc = client_update(global_net, train_dataloaders[client_index], E, lr,device)\n",
    "            client_nets.append(client_net)\n",
    "            length = len(train_datasets[client_index]) # example number\n",
    "            accumulater.add(train_loss*length,train_acc*length,length)\n",
    "\n",
    "        global_net_zero(global_net)  # make global net params all zero\n",
    "        n = 0 # get n. n should be the sum of examples in variable client_nets, not 60000\n",
    "        for client_index in client_indexs:\n",
    "            n += len(train_datasets[client_index]) # example number\n",
    "        for client_index in client_indexs:  # update global net\n",
    "            update_global_net(global_net, client_nets[client_indexs.index(client_index)], n, len(train_datasets[client_index]))\n",
    "\n",
    "        # check whether test acc reach the target\n",
    "        test_acc = get_test_acc(global_net, test_dataloader,device)\n",
    "        step += 1\n",
    "        \n",
    "        writer.add_scalar(\"train loss\", accumulater[0] / accumulater[2], step)\n",
    "        writer.add_scalar(\"train acc\", accumulater[1] / accumulater[2], step)\n",
    "        writer.add_scalar(\"test acc\", test_acc, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IID_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/whr-pc-ubuntu/code/fedavg/table_1_1.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B3060ti/home/whr-pc-ubuntu/code/fedavg/table_1_1.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m IID_train(C_list, E,B, lr, seed, train_dataloaders, test_dataset, test_acc_target, client_number)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IID_train' is not defined"
     ]
    }
   ],
   "source": [
    "IID_train(C_list, E,B, lr, seed, train_dataloaders, test_dataset, test_acc_target, client_number)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B = inf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 'inf'  # batch size for all clients\n",
    "# 加载数据，当B变化时，数据不同\n",
    "train_dataloaders = [DataLoader(train_dataset, len(train_dataset) if B == 'inf' else B,\n",
    "                                shuffle=False) for train_dataset in train_datasets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = d2l.try_gpu()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in C_list:\n",
    "    writer = get_writer('MNIST_2NN', 'IID', f'B={B}', f'C={C}')\n",
    "    global_net = MNIST_2NN()\n",
    "    global_net.init_params(seed)\n",
    "    global_net.to(device)\n",
    "    step = 0\n",
    "    test_acc = 0\n",
    "    while test_acc < test_acc_target:  # control the variable t by the acc target\n",
    "        m = max(int(C*client_number), 1)\n",
    "        client_indexs = random.sample(range(0, client_number), m)  # select m clients randomly\n",
    "\n",
    "        client_nets = []  # store net(t+1,client_index) by local update\n",
    "        accumulater = d2l.Accumulator(3)\n",
    "        for client_index in client_indexs:\n",
    "            client_net,train_loss,train_acc = client_update(global_net, train_dataloaders[client_index], E, lr,device)\n",
    "            client_nets.append(client_net)\n",
    "            length = len(train_datasets[client_index]) # example number\n",
    "            accumulater.add(train_loss*length,train_acc*length,length)\n",
    "\n",
    "        global_net_zero(global_net)  # make global net params all zero\n",
    "        n = 0 # get n. n should be the sum of examples in variable client_nets, not 60000\n",
    "        for client_index in client_indexs:\n",
    "            n += len(train_datasets[client_index]) # example number\n",
    "        for client_index in client_indexs:  # update global net\n",
    "            update_global_net(global_net, client_nets[client_indexs.index(client_index)], n, len(train_datasets[client_index]))\n",
    "\n",
    "        # check whether test acc reach the target\n",
    "        test_acc = get_test_acc(global_net, test_dataloader,device)\n",
    "        step += 1\n",
    "        \n",
    "        writer.add_scalar(\"train loss\", accumulater[0] / accumulater[2], step)\n",
    "        writer.add_scalar(\"train acc\", accumulater[1] / accumulater[2], step)\n",
    "        writer.add_scalar(\"test acc\", test_acc, step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
