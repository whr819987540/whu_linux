{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二维卷积计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x: torch.Tensor, k: torch.Tensor): #@save\n",
    "    \"\"\"\n",
    "        二维卷积计算\n",
    "    \"\"\"\n",
    "    h, w = k.shape\n",
    "    y = torch.zeros((x.shape[0]-h+1, x.shape[1]-w+1))\n",
    "\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            y[i, j] = (x[i:i+h, j:j+w]*k).sum()\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 1, 2],\n",
    "                  [3, 4, 5],\n",
    "                  [6, 7, 8]])\n",
    "k = torch.tensor([[0, 1],\n",
    "                  [2, 3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = conv2d(x,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 水平边缘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((5,6))\n",
    "x[2:4,:] = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.tensor([[1],[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [-1., -1., -1., -1., -1., -1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d(x,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d(x,k).abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 垂直边缘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((6,8))\n",
    "x[:,2:6] = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.tensor([[1,-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d(x,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d(x,k).abs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练-垂直边缘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = conv2d(x,k)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape((1,1,y.shape[0],y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "          [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "          [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "          [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "          [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "          [1., 1., 0., 0., 0., 0., 1., 1.]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size c h w\n",
    "x = x.reshape((1,1,x.shape[0],x.shape[1]))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2D = nn.Conv2d(1,1,(1,2),bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "epoch = 200\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch 0 loss 0.8351865410804749---\n",
      "--- epoch 1 loss 0.7853633165359497---\n",
      "--- epoch 2 loss 0.7393738627433777---\n",
      "--- epoch 3 loss 0.6969024538993835---\n",
      "--- epoch 4 loss 0.6576598286628723---\n",
      "--- epoch 5 loss 0.6213808655738831---\n",
      "--- epoch 6 loss 0.5878223776817322---\n",
      "--- epoch 7 loss 0.5567620396614075---\n",
      "--- epoch 8 loss 0.5279955267906189---\n",
      "--- epoch 9 loss 0.5013357400894165---\n",
      "--- epoch 10 loss 0.4766111969947815---\n",
      "--- epoch 11 loss 0.45366472005844116---\n",
      "--- epoch 12 loss 0.43235230445861816---\n",
      "--- epoch 13 loss 0.41254180669784546---\n",
      "--- epoch 14 loss 0.3941120505332947---\n",
      "--- epoch 15 loss 0.37695202231407166---\n",
      "--- epoch 16 loss 0.3609600067138672---\n",
      "--- epoch 17 loss 0.3460427522659302---\n",
      "--- epoch 18 loss 0.33211439847946167---\n",
      "--- epoch 19 loss 0.3190966844558716---\n",
      "--- epoch 20 loss 0.30691760778427124---\n",
      "--- epoch 21 loss 0.2955110967159271---\n",
      "--- epoch 22 loss 0.28481659293174744---\n",
      "--- epoch 23 loss 0.2747785449028015---\n",
      "--- epoch 24 loss 0.26534605026245117---\n",
      "--- epoch 25 loss 0.25647231936454773---\n",
      "--- epoch 26 loss 0.24811449646949768---\n",
      "--- epoch 27 loss 0.24023310840129852---\n",
      "--- epoch 28 loss 0.23279210925102234---\n",
      "--- epoch 29 loss 0.22575832903385162---\n",
      "--- epoch 30 loss 0.21910126507282257---\n",
      "--- epoch 31 loss 0.2127930074930191---\n",
      "--- epoch 32 loss 0.20680782198905945---\n",
      "--- epoch 33 loss 0.20112216472625732---\n",
      "--- epoch 34 loss 0.19571439921855927---\n",
      "--- epoch 35 loss 0.19056454300880432---\n",
      "--- epoch 36 loss 0.18565434217453003---\n",
      "--- epoch 37 loss 0.18096695840358734---\n",
      "--- epoch 38 loss 0.17648696899414062---\n",
      "--- epoch 39 loss 0.17220011353492737---\n",
      "--- epoch 40 loss 0.16809332370758057---\n",
      "--- epoch 41 loss 0.1641545444726944---\n",
      "--- epoch 42 loss 0.16037270426750183---\n",
      "--- epoch 43 loss 0.15673761069774628---\n",
      "--- epoch 44 loss 0.1532398760318756---\n",
      "--- epoch 45 loss 0.14987079799175262---\n",
      "--- epoch 46 loss 0.1466224193572998---\n",
      "--- epoch 47 loss 0.14348739385604858---\n",
      "--- epoch 48 loss 0.14045891165733337---\n",
      "--- epoch 49 loss 0.13753068447113037---\n",
      "--- epoch 50 loss 0.13469699025154114---\n",
      "--- epoch 51 loss 0.1319524198770523---\n",
      "--- epoch 52 loss 0.12929204106330872---\n",
      "--- epoch 53 loss 0.12671129405498505---\n",
      "--- epoch 54 loss 0.12420599907636642---\n",
      "--- epoch 55 loss 0.1217721551656723---\n",
      "--- epoch 56 loss 0.11940615624189377---\n",
      "--- epoch 57 loss 0.1171046644449234---\n",
      "--- epoch 58 loss 0.11486456543207169---\n",
      "--- epoch 59 loss 0.11268290132284164---\n",
      "--- epoch 60 loss 0.11055704206228256---\n",
      "--- epoch 61 loss 0.10848444700241089---\n",
      "--- epoch 62 loss 0.10646278411149979---\n",
      "--- epoch 63 loss 0.10448987036943436---\n",
      "--- epoch 64 loss 0.10256368666887283---\n",
      "--- epoch 65 loss 0.10068236291408539---\n",
      "--- epoch 66 loss 0.09884406626224518---\n",
      "--- epoch 67 loss 0.09704722464084625---\n",
      "--- epoch 68 loss 0.09529024362564087---\n",
      "--- epoch 69 loss 0.09357167035341263---\n",
      "--- epoch 70 loss 0.09189017117023468---\n",
      "--- epoch 71 loss 0.09024442732334137---\n",
      "--- epoch 72 loss 0.08863326162099838---\n",
      "--- epoch 73 loss 0.0870555117726326---\n",
      "--- epoch 74 loss 0.08551014959812164---\n",
      "--- epoch 75 loss 0.08399613946676254---\n",
      "--- epoch 76 loss 0.08251253515481949---\n",
      "--- epoch 77 loss 0.08105844259262085---\n",
      "--- epoch 78 loss 0.07963298261165619---\n",
      "--- epoch 79 loss 0.07823538780212402---\n",
      "--- epoch 80 loss 0.07686485350131989---\n",
      "--- epoch 81 loss 0.07552065700292587---\n",
      "--- epoch 82 loss 0.07420209795236588---\n",
      "--- epoch 83 loss 0.07290852069854736---\n",
      "--- epoch 84 loss 0.0716392993927002---\n",
      "--- epoch 85 loss 0.07039380073547363---\n",
      "--- epoch 86 loss 0.0691714659333229---\n",
      "--- epoch 87 loss 0.06797174364328384---\n",
      "--- epoch 88 loss 0.06679408252239227---\n",
      "--- epoch 89 loss 0.06563800573348999---\n",
      "--- epoch 90 loss 0.06450299173593521---\n",
      "--- epoch 91 loss 0.06338856369256973---\n",
      "--- epoch 92 loss 0.06229430064558983---\n",
      "--- epoch 93 loss 0.061219725757837296---\n",
      "--- epoch 94 loss 0.0601644441485405---\n",
      "--- epoch 95 loss 0.059128038585186005---\n",
      "--- epoch 96 loss 0.058110110461711884---\n",
      "--- epoch 97 loss 0.05711029842495918---\n",
      "--- epoch 98 loss 0.05612820014357567---\n",
      "--- epoch 99 loss 0.05516348406672478---\n",
      "--- epoch 100 loss 0.05421578884124756---\n",
      "--- epoch 101 loss 0.05328478291630745---\n",
      "--- epoch 102 loss 0.052370134741067886---\n",
      "--- epoch 103 loss 0.0514715276658535---\n",
      "--- epoch 104 loss 0.05058865249156952---\n",
      "--- epoch 105 loss 0.04972120001912117---\n",
      "--- epoch 106 loss 0.04886888712644577---\n",
      "--- epoch 107 loss 0.04803143069148064---\n",
      "--- epoch 108 loss 0.04720854386687279---\n",
      "--- epoch 109 loss 0.04639995098114014---\n",
      "--- epoch 110 loss 0.04560540243983269---\n",
      "--- epoch 111 loss 0.04482462257146835---\n",
      "--- epoch 112 loss 0.044057365506887436---\n",
      "--- epoch 113 loss 0.043303389102220535---\n",
      "--- epoch 114 loss 0.04256244748830795---\n",
      "--- epoch 115 loss 0.04183429852128029---\n",
      "--- epoch 116 loss 0.04111871123313904---\n",
      "--- epoch 117 loss 0.04041546955704689---\n",
      "--- epoch 118 loss 0.039724331349134445---\n",
      "--- epoch 119 loss 0.03904511407017708---\n",
      "--- epoch 120 loss 0.038377586752176285---\n",
      "--- epoch 121 loss 0.03772154077887535---\n",
      "--- epoch 122 loss 0.03707676753401756---\n",
      "--- epoch 123 loss 0.0364430770277977---\n",
      "--- epoch 124 loss 0.03582027554512024---\n",
      "--- epoch 125 loss 0.03520815819501877---\n",
      "--- epoch 126 loss 0.03460654616355896---\n",
      "--- epoch 127 loss 0.034015264362096786---\n",
      "--- epoch 128 loss 0.033434126526117325---\n",
      "--- epoch 129 loss 0.03286294639110565---\n",
      "--- epoch 130 loss 0.03230155631899834---\n",
      "--- epoch 131 loss 0.03174978494644165---\n",
      "--- epoch 132 loss 0.03120746836066246---\n",
      "--- epoch 133 loss 0.030674435198307037---\n",
      "--- epoch 134 loss 0.030150528997182846---\n",
      "--- epoch 135 loss 0.02963559702038765---\n",
      "--- epoch 136 loss 0.02912946604192257---\n",
      "--- epoch 137 loss 0.02863200753927231---\n",
      "--- epoch 138 loss 0.028143052011728287---\n",
      "--- epoch 139 loss 0.02766246721148491---\n",
      "--- epoch 140 loss 0.02719010040163994---\n",
      "--- epoch 141 loss 0.02672581933438778---\n",
      "--- epoch 142 loss 0.026269469410181046---\n",
      "--- epoch 143 loss 0.02582092583179474---\n",
      "--- epoch 144 loss 0.025380048900842667---\n",
      "--- epoch 145 loss 0.024946706369519234---\n",
      "--- epoch 146 loss 0.024520771577954292---\n",
      "--- epoch 147 loss 0.024102117866277695---\n",
      "--- epoch 148 loss 0.02369062416255474---\n",
      "--- epoch 149 loss 0.023286154493689537---\n",
      "--- epoch 150 loss 0.02288859710097313---\n",
      "--- epoch 151 loss 0.022497836500406265---\n",
      "--- epoch 152 loss 0.022113747894763947---\n",
      "--- epoch 153 loss 0.02173621952533722---\n",
      "--- epoch 154 loss 0.02136513963341713---\n",
      "--- epoch 155 loss 0.02100040204823017---\n",
      "--- epoch 156 loss 0.020641891285777092---\n",
      "--- epoch 157 loss 0.020289510488510132---\n",
      "--- epoch 158 loss 0.01994314417243004---\n",
      "--- epoch 159 loss 0.019602693617343903---\n",
      "--- epoch 160 loss 0.019268052652478218---\n",
      "--- epoch 161 loss 0.018939130008220673---\n",
      "--- epoch 162 loss 0.01861582323908806---\n",
      "--- epoch 163 loss 0.018298039212822914---\n",
      "--- epoch 164 loss 0.01798567920923233---\n",
      "--- epoch 165 loss 0.017678657546639442---\n",
      "--- epoch 166 loss 0.017376871779561043---\n",
      "--- epoch 167 loss 0.017080241814255714---\n",
      "--- epoch 168 loss 0.016788676381111145---\n",
      "--- epoch 169 loss 0.01650209352374077---\n",
      "--- epoch 170 loss 0.016220400109887123---\n",
      "--- epoch 171 loss 0.015943512320518494---\n",
      "--- epoch 172 loss 0.01567135751247406---\n",
      "--- epoch 173 loss 0.015403847210109234---\n",
      "--- epoch 174 loss 0.015140902251005173---\n",
      "--- epoch 175 loss 0.014882447198033333---\n",
      "--- epoch 176 loss 0.014628404751420021---\n",
      "--- epoch 177 loss 0.014378699474036694---\n",
      "--- epoch 178 loss 0.014133253134787083---\n",
      "--- epoch 179 loss 0.013892003335058689---\n",
      "--- epoch 180 loss 0.013654868118464947---\n",
      "--- epoch 181 loss 0.013421784155070782---\n",
      "--- epoch 182 loss 0.013192678801715374---\n",
      "--- epoch 183 loss 0.012967484071850777---\n",
      "--- epoch 184 loss 0.012746133841574192---\n",
      "--- epoch 185 loss 0.012528562918305397---\n",
      "--- epoch 186 loss 0.012314705178141594---\n",
      "--- epoch 187 loss 0.012104498222470284---\n",
      "--- epoch 188 loss 0.011897876858711243---\n",
      "--- epoch 189 loss 0.01169478427618742---\n",
      "--- epoch 190 loss 0.011495159938931465---\n",
      "--- epoch 191 loss 0.011298942379653454---\n",
      "--- epoch 192 loss 0.011106075718998909---\n",
      "--- epoch 193 loss 0.01091650128364563---\n",
      "--- epoch 194 loss 0.010730160400271416---\n",
      "--- epoch 195 loss 0.010547000914812088---\n",
      "--- epoch 196 loss 0.010366967879235744---\n",
      "--- epoch 197 loss 0.010190004482865334---\n",
      "--- epoch 198 loss 0.010016067884862423---\n",
      "--- epoch 199 loss 0.009845100343227386---\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    y_hat = conv2D(x)\n",
    "    loss = loss_func(y_hat,y)\n",
    "    # loss = torch.square((y_hat-y)).sum()\n",
    "    # 均方差损失函数没有直接求方差的效果好\n",
    "    # 在lr相同时，需要调大epoch\n",
    "    # 也可以直接调大lr\n",
    "    conv2D.zero_grad()\n",
    "    loss.backward()\n",
    "    conv2D.weight.data[:] = conv2D.weight.data[:] - lr*conv2D.weight.grad\n",
    "\n",
    "    print(f'--- epoch {i} loss {loss}---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.8161, -0.8159]]]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2D.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((8,8))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((1,1)+x.shape)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2D = nn.Conv2d(1,1,(5,5))\n",
    "y = conv2D(x)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2D = nn.Conv2d(1,1,(5,5),padding=(2,2))\n",
    "y = conv2D(x)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 6, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2D = nn.Conv2d(1,1,(3,5))\n",
    "y = conv2D(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2D = nn.Conv2d(1,1,(3,5),padding=(1,2))\n",
    "y = conv2D(x)\n",
    "y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多输入通道"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个channel先单独计算卷积, 然后求和\n",
    "\n",
    "输入是两个通道, 输出仍是一个通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6],\n",
    "     [7, 8, 9]],\n",
    "    [[0, 1, 2],\n",
    "     [3, 4, 5],\n",
    "     [6, 7, 8]]])\n",
    "\n",
    "k = torch.tensor(\n",
    "    [[[1,2],\n",
    "      [3,4]],\n",
    "     [[0,1],\n",
    "      [2,3]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3]), torch.Size([2, 2, 2]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_multi_in(x, w):\n",
    "    channels = x.shape[0]\n",
    "    y = torch.zeros((channels, x.shape[1]-w.shape[1]+1, x.shape[2]-w.shape[2]+1))\n",
    "    for channel in range(y.shape[0]):\n",
    "        for i in range(y.shape[1]):\n",
    "            for j in range(y.shape[2]):\n",
    "                y[channel, i, j] = (x[channel, i:i+w.shape[1], j:j+w.shape[2]] * w[channel,:,:]).sum()\n",
    "                # print(channel,i,j,y[channel,i,j])\n",
    "\n",
    "    res = torch.zeros((y.shape[1], y.shape[2]))\n",
    "    for channel in range(y.shape[0]):\n",
    "        res += y[channel, :, :]\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_multi_in(x,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多输出通道"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    O_o*O_i*k_h*k_w\n",
    "    \\\\\n",
    "    输出通道*输入通道*卷积核形状\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.stack([k,k+1,k+2])\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 2],\n",
       "          [3, 4]],\n",
       "\n",
       "         [[0, 1],\n",
       "          [2, 3]]],\n",
       "\n",
       "\n",
       "        [[[2, 3],\n",
       "          [4, 5]],\n",
       "\n",
       "         [[1, 2],\n",
       "          [3, 4]]],\n",
       "\n",
       "\n",
       "        [[[3, 4],\n",
       "          [5, 6]],\n",
       "\n",
       "         [[2, 3],\n",
       "          [4, 5]]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_multi_in_out(x,k):\n",
    "    # i in k 遍历k的每一个通道（三维张量）\n",
    "    # torch.stack将每个计算结果都连接起来\n",
    "    return torch.stack([conv2d_multi_in(x,i) for i in k ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_multi_in_out(x,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汇聚层"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大汇聚层, 将某个形状窗口中的最大值作为结果\n",
    "\n",
    "平均汇聚层, 将某个形状窗口中的平均值作为结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(x,pool_size,mode='max'):\n",
    "    y = torch.ones((x.shape[0]-pool_size[0]+1,x.shape[0]-pool_size[1]+1))\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                y[i,j] = x[i:i+pool_size[0],j:j+pool_size[1]].max()\n",
    "            else:\n",
    "                y[i,j] = x[i:i+pool_size[0],j:j+pool_size[1]].mean()\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [0,1,2,],\n",
    "    [3,4,5,],\n",
    "    [6,7,8,],\n",
    "    ],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(x,(2,2),'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(x,(2,2),'avg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae68aafe2bef47f5634c2740228d6359f2100640f7deafc9662b2060061f7b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
