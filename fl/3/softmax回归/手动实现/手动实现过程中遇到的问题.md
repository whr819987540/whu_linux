# 问题1

epoch为0，在使用第二组数据时，y_hat中很多nan，然后参数更新时，w、b都变成了nan，所以后面的y_hat也都变成了nan。

因为w、b都变成了nan，所以后面就保持不变了。

![image-20221028201338980](https://raw.githubusercontent.com/whr819987540/pic/main/image-20221028201338980.png)

首先看为什么y_hat中很多nan，是因为执行了softmax函数。

对于softmax，如果xi很大，e^xi趋近于正无穷。正无穷出现在分子或分母上，导致y_hat中出现了很多inf、nan。

![image-20221028201435129](https://raw.githubusercontent.com/whr819987540/pic/main/image-20221028201435129.png)

**这不是理论问题，而是计算问题。**

## 解决

应该解决softmax函数的实现。
$$
softmax : \hat y_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
$$
由于o<sub>j</sub>可能非常大（**上溢**），所以上下同时提取下式：
$$
\exp(max(o_k))
$$
得到：
$$
\begin{aligned}
\hat y_j &= \frac{\exp(o_j)}{\sum_k \exp(o_k)} \\
& =  \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\
& = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.
\end{aligned}
$$
但如果o<sub>j</sub>-max(o<sub>k</sub>)极小，分子为0，则值为nan（**下溢**）。

继续考虑损失函数（交叉熵损失函数）：
$$
loss : 
\begin{aligned}
-\log{(\hat y_j)} & = -\log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\
& = \log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} - \log{(\exp(o_j - \max(o_k)))} \\
& = \max(o_k) + \log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} - o_j.
\end{aligned}
$$
这样就避免了在计算的中间过程中，因为精度不够而引入nan、inf，使得反向传播出现问题。

所以，最终的解决方法是，在模型中，直接计算 $\hat y_i = x_iW + b$，不单独将$ \hat y_i$输入softmax函数，而是输入到上述新的loss函数中。

这个函数的实现是`nn.CrossEntropyLoss(*reduction*='none')`。

# 问题2

W初始值的正态分布曲线中，sigma应该设置得小一点。

## sigma=1

![image-20221028200954382](https://raw.githubusercontent.com/whr819987540/pic/main/image-20221028200954382.png)



![image-20221028201257749](https://raw.githubusercontent.com/whr819987540/pic/main/image-20221028201257749.png)

## sigma=0.01

![image-20221028200903406](https://raw.githubusercontent.com/whr819987540/pic/main/image-20221028200903406.png)



![image-20221028200838612](https://raw.githubusercontent.com/whr819987540/pic/main/image-20221028200838612.png)





$$

$$



$$
\begin{align}
  H^{(1)} &= \sigma(XW^{(1)}+b^{(1)}) \\
  X&\in R^{n*784} \\
  W^{(1)}&\in R^{784*256} \\
  b^{(1)}&\in R^{1*256} \\
  H^{(1)}&\in R^{n*256} \\
  \\
  
  \sigma(x) &=ReLU(x) \\
  			&=max(x,0) \\
\end{align}
$$

$$
\begin{align}
  
  \hat Y &= H^{(1)}W^{(2)} + b^{(2)} \\
  H^{(1)}&\in R^{n*256} \\
  W^{(2)}&\in R^{256*10} \\
  b^{(2)}&\in R^{1*10}\\
  \hat Y &\in R^{n*10}
  
  \end{align}
$$

$$
\begin{align}

MSLoss(\lg\hat y, \lg y) &= \frac {1} {n} \sum\limits_{i=1}^{i=n} (\lg \hat y_{i} - \lg {y_i})^{2}

\end{align}
$$







$$
\begin{aligned}

	h' = \begin{cases}
		0&, p \\
		\frac {h} {1-p}&,1-p
		\end{cases}

\end{aligned}
$$

$$
\begin{align}
y_k= \sum _{i,j} W_{ijk} x_{i} x_{j}
\end{align}
$$




