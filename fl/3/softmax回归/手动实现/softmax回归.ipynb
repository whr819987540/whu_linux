{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装\n",
    "def load_data_fashion_mnist(batch_size):\n",
    "    trans = transforms.ToTensor()\n",
    "    # 下载数据\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../../../data/\", train=True, download=True, transform=trans\n",
    "    )\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../../../data/\", train=False, download=True, transform=trans\n",
    "    )\n",
    "    # 加载数据\n",
    "    mnist_train_data_loader = data.DataLoader(\n",
    "        mnist_train, batch_size=batch_size, shuffle=True)\n",
    "    mnist_test_data_loader = data.DataLoader(\n",
    "        mnist_test, batch_size=batch_size, shuffle=True)\n",
    "    return mnist_train_data_loader, mnist_test_data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader,test_data_loader = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx,ty = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 1, 28, 28]), torch.Size([256]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx.shape, ty.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始输入是一张28\\*28的图像, 将其展成一维向量, 长度是28\\*28=784\n",
    "\n",
    "所以最终的输入是长度为784的向量\n",
    "\n",
    "考虑回归方程:\n",
    "$$\n",
    "\n",
    "\\begin{align} \n",
    "\ty_0 = w_{0,0}*x_0 + w_{0,1}*x_1 + ... + w_{0,783}*x_{783} + b_0 \\\\\n",
    "\n",
    "    y_1 = w_{1,0}*x_0 + w_{1,1}*x_1 + ... + w_{0,783}*x_{783} + b_1 \\\\\n",
    "\n",
    "    ... \\\\\n",
    "\n",
    "    y_9 = w_{9,0}*x_0 + w_{9,1}*x_1 + ... + w_{0,783}*x_{783} + b_9 \\\\\n",
    "\n",
    "    Y = XW + b \\\\\n",
    "    \n",
    "    X是1*784，W是784*10，b是1*10\n",
    "\\end{align}\n",
    "\n",
    "$$\n",
    "\n",
    "所以最终的输出是长度为10的向量1*10\n",
    "\n",
    "但这个输出不是概率，也不符合概率的性质（非负，和为1），所以需要进行归一化\n",
    "\n",
    "采用的方法是 output(i) = e^yi/∑e^yi，保证输出符合概率的性质，且不影响yi的大小关系\n",
    "\n",
    "参数w是784\\*10的矩阵, b是1\\*10的矩阵\n",
    "\n",
    "**但实际训练或测试时,一次可能不止输入一张图片,所以x其实是n*784的矩阵**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "# w = torch.normal(0,1,(input_size,output_size),requires_grad=True)\n",
    "w = torch.normal(0,0.01,(input_size,output_size),requires_grad=True) # sigma应该比较小\n",
    "b = torch.zeros((output_size,),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 10]), torch.Size([10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0126,  0.0057, -0.0181,  ...,  0.0172,  0.0156, -0.0069],\n",
       "         [ 0.0016,  0.0056,  0.0043,  ...,  0.0257, -0.0030, -0.0062],\n",
       "         [ 0.0008,  0.0052, -0.0082,  ..., -0.0049, -0.0005, -0.0019],\n",
       "         ...,\n",
       "         [ 0.0141,  0.0009,  0.0011,  ...,  0.0008, -0.0067, -0.0145],\n",
       "         [ 0.0021,  0.0102,  0.0037,  ..., -0.0054, -0.0077, -0.0085],\n",
       "         [ 0.0029, -0.0085,  0.0089,  ..., -0.0124,  0.0083,  0.0063]],\n",
       "        requires_grad=True),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    output(y_i) = \\frac{e^{y_i}}{∑e^{y_k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数理论上没有问题, 可以将$y_i$转化为非负数, 小于等于1, 且和为1\n",
    "\n",
    "但是计算上, 因为计算机的精度有限, 所以可能上溢或下溢, 影响反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
       "         0.9000],\n",
       "        [1.0000, 1.1000, 1.2000, 1.3000, 1.4000, 1.5000, 1.6000, 1.7000, 1.8000,\n",
       "         1.9000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.arange(0,2,step=0.1).reshape((2,10))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.1052, 1.2214, 1.3499, 1.4918, 1.6487, 1.8221, 2.0138, 2.2255,\n",
       "         2.4596],\n",
       "        [2.7183, 3.0042, 3.3201, 3.6693, 4.0552, 4.4817, 4.9530, 5.4739, 6.0496,\n",
       "         6.6859]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_e = torch.exp(tmp)\n",
    "tmp_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16.3380],\n",
       "        [44.4113]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_e.sum(dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0612, 0.0676, 0.0748, 0.0826, 0.0913, 0.1009, 0.1115, 0.1233, 0.1362,\n",
       "         0.1505],\n",
       "        [0.0612, 0.0676, 0.0748, 0.0826, 0.0913, 0.1009, 0.1115, 0.1233, 0.1362,\n",
       "         0.1505]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = tmp_e / tmp_e.sum(dim=1,keepdim=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.sum(dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0612, 0.0676, 0.0748, 0.0826, 0.0913, 0.1009, 0.1115, 0.1233, 0.1362,\n",
       "         0.1505],\n",
       "        [0.0612, 0.0676, 0.0748, 0.0826, 0.0913, 0.1009, 0.1115, 0.1233, 0.1362,\n",
       "         0.1505]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(torch.arange(0,2,0.1).reshape((2,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = softmax(torch.normal(0,1,(4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1861, 0.0533, 0.0542, 0.1478, 0.5586],\n",
       "        [0.2171, 0.4910, 0.0676, 0.0303, 0.1941],\n",
       "        [0.1673, 0.1502, 0.1865, 0.4454, 0.0505],\n",
       "        [0.1698, 0.0164, 0.0631, 0.3146, 0.4360]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~yi = xWi + bi~~\n",
    "\n",
    "~~output(i) = e^yi/∑e^yi~~\n",
    "\n",
    "经过对softmax函数计算不可行的分析,上面的计算模型也不可行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax_regression(x):\n",
    "#     # x 可能是1*784的向量，也可能是n*784的向量，具体取决于batch_size\n",
    "#     # 所以不能写成 y = torch.matmul(x.reshape(1,-1),w) + b\n",
    "#     y = torch.matmul(x.reshape(-1,w.shape[0]),w) + b\n",
    "#     return softmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression(x):\n",
    "    # x 可能是1*784的向量，也可能是n*784的向量，具体取决于batch_size\n",
    "    # 所以不能写成 y = torch.matmul(x.reshape(1,-1),w) + b\n",
    "    y = torch.matmul(x.reshape(-1,w.shape[0]),w) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_hat是通过softmax函数计算出的概率矩阵\n",
    "    \n",
    "y是标签（类别）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y表示正确的类别，所以先选取对应的概率\n",
    "\n",
    "因为目标是使概率p最大、损失函数f最小，所以应该加上负数，变成-p\n",
    "\n",
    "又因为-p是负数，不方便用梯度；且 0=<p<=1，所以取 -logp\n",
    "\n",
    "目标是使-logp接近于0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5000, 0.8000, 0.5000, 0.4000, 0.9000],\n",
       "         [0.6000, 0.5000, 0.1000, 0.9000, 0.9000],\n",
       "         [0.4000, 0.9000, 0.8000, 0.7000, 0.2000],\n",
       "         [0.7000, 0.6000, 0.5000, 0.4000, 0.3000]]),\n",
       " tensor([0, 3]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.randint(0,10,(4,5))/10\n",
    "y = torch.tensor([0,3])\n",
    "y_hat,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.4000],\n",
       "        [0.6000, 0.9000],\n",
       "        [0.4000, 0.7000],\n",
       "        [0.7000, 0.4000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0:len(y_hat),y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6931, 0.9163],\n",
       "        [0.5108, 0.1054],\n",
       "        [0.9163, 0.3567],\n",
       "        [0.3567, 0.9163]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- torch.log(y_hat[0:len(y_hat),y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_function(y_hat,y):\n",
    "    return - torch.log(y_hat[0:len(y_hat),y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度评价算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_hat的列表示预测某一张图片属于第i个列的概率大小\n",
    "\n",
    "从所有列中选出最大值，获得i，与标签y中的值进行比较\n",
    "\n",
    "如果相同，表示预测正确；否则预测失败\n",
    "\n",
    "精度accuracy = 预测正确次数/总的预测次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6000, 0.1000, 0.8000, 0.8000, 0.7000],\n",
       "        [0.7000, 0.8000, 0.0000, 0.1000, 0.9000],\n",
       "        [0.0000, 0.1000, 0.5000, 0.1000, 0.8000],\n",
       "        [0.1000, 0.5000, 0.4000, 0.1000, 0.9000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.randint(0,10,(4,5)) / 10\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2, 2, 0, 3]]), torch.Size([1, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([2,2,0,3]).reshape(1,-1)\n",
    "y,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2],\n",
       "         [4],\n",
       "         [4],\n",
       "         [4]]),\n",
       " torch.Size([4, 1]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = y_hat.max(dim=1,keepdim=True)\n",
    "t.indices, t.indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 4, 4]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.indices.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t.indices.T == y).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 精度计算过程封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat,y):\n",
    "    \"\"\"\n",
    "        返回正确的预测数与总的预测数\n",
    "    \"\"\"\n",
    "    y_hat_max = y_hat.max(dim=1,keepdim=True)\n",
    "    index = y_hat_max.indices.T\n",
    "    index.type(y.dtype)\n",
    "    # print(int((index == y).sum()),y.shape[0])\n",
    "    return int((index == y).sum()),y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_hat,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 记录正确的预测数与总的预测次数等信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self,n) -> None:\n",
    "        self.data = [0.0]*n\n",
    "    \n",
    "    def add(self,*args):\n",
    "        for i,arg in enumerate(args):\n",
    "            self.data[i] += arg\n",
    "            \n",
    "    def reset(self):\n",
    "        self.data = [0.0]*len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_evaluate(net,data):\n",
    "    evaluator = Accumulator(2) # 正确与总数\n",
    "    with torch.no_grad(): # 不计算梯度，更快\n",
    "        for x,y in data:\n",
    "            acc = accuracy(net(x),y)\n",
    "            evaluator.add(acc[0],acc[1]) # accuracy的计算返回元组\n",
    "    \n",
    "    return 1.0 * evaluator[0] / evaluator[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13291666666666666"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "accuracy_evaluate(softmax_regression,train_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果应该接近于1/10, 因为没有对模型参数进行修正, 相当于随机分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还是用随机梯度下降来进行模型参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updater(params,lr,batch_size):\n",
    "    sgd([w,b],lr,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updater = torch.optim.SGD([w,b],lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单次迭代函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,data,loss_function,updater):\n",
    "    # 记录loss，正确预测数，总的预测数\n",
    "    evaluator = Accumulator(3)\n",
    "    for x,y in data:\n",
    "        # 预测值\n",
    "        y_hat = net(x)\n",
    "        l = loss_function(y_hat,y)\n",
    "        # 更新模型参数\n",
    "        if isinstance(updater,torch.optim.Optimizer):\n",
    "            # pytorch内置优化器\n",
    "            updater.zero_grad()\n",
    "            l.sum().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            # print(b)\n",
    "            l.sum().backward()\n",
    "            updater([w,b],lr,batch_size)\n",
    "            # print(b)\n",
    "            \n",
    "        acc = accuracy(y_hat,y)\n",
    "        evaluator.add(l.sum(),acc[0],acc[1])\n",
    "    # loss，acc\n",
    "    # print(evaluator[0],evaluator[1],evaluator[2])\n",
    "    return float(evaluator[0] / evaluator[2]), evaluator[1] / evaluator[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总的迭代函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_data,test_data,loss_function,updater,epochs):\n",
    "    print(\"epoch\\tloss\\ttrain_acc\\ttest_acc\")\n",
    "    for epoch in range(epochs):\n",
    "        loss ,train_acc = train_epoch(net,train_data,loss_function,updater)\n",
    "        test_acc = accuracy_evaluate(net,test_data)\n",
    "        print(f\"{epoch}\\t{round(loss,5)}\\t{round(train_acc,5)}\\t\\t{round(test_acc,5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax函数的选择问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\tloss\ttrain_acc\ttest_acc\n",
      "0\t0.7868\t0.74838\t\t0.7909\n",
      "1\t0.57116\t0.8129\t\t0.8119\n",
      "2\t0.52586\t0.82522\t\t0.8169\n",
      "3\t0.50055\t0.8332\t\t0.8236\n",
      "4\t0.48613\t0.83532\t\t0.8264\n",
      "5\t0.47354\t0.84005\t\t0.8262\n",
      "6\t0.46509\t0.84288\t\t0.8303\n",
      "7\t0.45892\t0.84422\t\t0.8329\n",
      "8\t0.45178\t0.84697\t\t0.8298\n",
      "9\t0.4478\t0.84808\t\t0.8325\n"
     ]
    }
   ],
   "source": [
    "train(softmax_regression,train_data_loader,test_data_loader,nn.CrossEntropyLoss(reduction='none'),updater,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\tloss\ttrain_acc\ttest_acc\n",
      "0\t1.9808\t0.56837\t\t0.6676\n",
      "1\t1.82432\t0.68242\t\t0.686\n",
      "2\t1.78427\t0.72687\t\t0.738\n",
      "3\t1.75579\t0.75528\t\t0.7537\n",
      "4\t1.73989\t0.76528\t\t0.7593\n",
      "5\t1.72952\t0.77112\t\t0.7623\n",
      "6\t1.72198\t0.7747\t\t0.7688\n",
      "7\t1.71608\t0.77833\t\t0.7727\n",
      "8\t1.71128\t0.78088\t\t0.7743\n",
      "9\t1.70733\t0.7834\t\t0.7766\n"
     ]
    }
   ],
   "source": [
    "train(softmax_regression,train_data_loader,test_data_loader,nn.CrossEntropyLoss(reduction='none'),updater,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 按照定义实现的softmax函数，存在计算问题\n",
    "# train(softmax_regression,train_data_loader,test_data_loader,cross_entropy_loss_function,updater,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签到文字的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_int_to_text(labels_int):\n",
    "    labels_text = ['t-shirt', 'trouser', 'pullover', 'dress',\n",
    "                   'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [labels_text[label_int] for label_int in labels_int]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1678e-05, 9.9884e-01, 5.2085e-06, 1.0889e-03, 4.3732e-05, 4.0397e-09,\n",
      "         4.3896e-07, 1.6934e-07, 2.3326e-07, 3.6351e-07],\n",
      "        [9.9993e-01, 7.2849e-07, 6.9031e-06, 5.8541e-05, 5.7826e-06, 1.2851e-10,\n",
      "         1.0127e-08, 2.3651e-10, 2.1574e-07, 2.3925e-07],\n",
      "        [3.0867e-05, 6.0178e-06, 6.5565e-02, 1.7331e-05, 9.3414e-01, 6.8961e-07,\n",
      "         5.0407e-07, 1.5158e-08, 2.3395e-04, 1.7595e-06],\n",
      "        [4.2225e-05, 1.4457e-07, 4.7238e-06, 3.3397e-06, 4.9963e-05, 1.5427e-03,\n",
      "         4.4332e-07, 4.2196e-06, 5.9217e-05, 9.9829e-01],\n",
      "        [5.2439e-03, 7.7306e-03, 2.8576e-03, 9.7778e-01, 3.2134e-03, 2.0351e-04,\n",
      "         7.6654e-04, 5.1421e-04, 7.7272e-04, 9.1442e-04],\n",
      "        [1.0731e-08, 7.1433e-10, 2.6312e-08, 3.3784e-09, 1.2203e-07, 1.0518e-03,\n",
      "         3.7678e-09, 8.6618e-05, 5.5761e-07, 9.9886e-01],\n",
      "        [3.8950e-06, 9.9183e-07, 1.2397e-05, 4.3025e-06, 1.2606e-04, 6.1044e-02,\n",
      "         6.3827e-06, 8.1774e-01, 7.0572e-04, 1.2036e-01],\n",
      "        [9.9968e-01, 1.7804e-05, 1.8927e-04, 9.1790e-05, 1.7048e-05, 4.5258e-08,\n",
      "         4.1754e-07, 7.9099e-09, 4.0451e-06, 5.0608e-07],\n",
      "        [3.5021e-04, 9.9786e-01, 3.3808e-05, 1.6322e-03, 1.1266e-04, 3.8643e-08,\n",
      "         1.8306e-06, 8.9632e-07, 2.0627e-07, 4.3968e-06],\n",
      "        [1.2692e-01, 1.5470e-03, 1.2496e-01, 4.5949e-02, 6.9879e-01, 8.1167e-06,\n",
      "         2.0615e-05, 6.6181e-06, 1.3869e-03, 4.1012e-04],\n",
      "        [1.3379e-04, 2.7789e-05, 9.9524e-01, 5.8364e-05, 3.9417e-03, 2.2458e-04,\n",
      "         2.0603e-05, 1.6243e-06, 3.2787e-04, 2.7431e-05],\n",
      "        [2.0107e-05, 9.9893e-01, 7.8024e-06, 9.5637e-04, 7.3422e-05, 1.8641e-08,\n",
      "         1.4647e-06, 1.3793e-06, 1.4892e-07, 8.1728e-06],\n",
      "        [2.6368e-03, 2.6243e-03, 1.5206e-05, 9.9401e-01, 1.0484e-04, 3.7210e-09,\n",
      "         1.7720e-06, 8.7152e-06, 1.6293e-05, 5.8514e-04],\n",
      "        [2.5843e-03, 7.6506e-08, 5.7124e-03, 3.4038e-06, 4.1035e-04, 1.4650e-04,\n",
      "         2.4732e-07, 6.0799e-08, 9.9112e-01, 2.6911e-05],\n",
      "        [2.2153e-04, 1.5653e-04, 1.7060e-03, 1.0998e-05, 1.5034e-03, 2.2248e-01,\n",
      "         5.1835e-04, 4.9795e-02, 1.1913e-02, 7.1169e-01],\n",
      "        [1.6390e-04, 9.9825e-01, 1.1318e-05, 1.5001e-03, 7.1001e-05, 1.1523e-09,\n",
      "         2.4533e-07, 3.4908e-07, 2.4839e-08, 2.6102e-06],\n",
      "        [1.6594e-02, 1.1480e-03, 1.3443e-06, 9.7944e-01, 8.9893e-05, 2.1754e-07,\n",
      "         1.9285e-06, 1.2755e-05, 2.5009e-03, 2.0618e-04],\n",
      "        [1.5168e-02, 1.1337e-03, 4.2399e-05, 9.8358e-01, 6.9551e-05, 5.3944e-10,\n",
      "         3.2505e-07, 2.1208e-07, 1.4962e-06, 2.6294e-06]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([1, 0, 4, 9, 3, 9, 7, 0, 1, 4, 2, 1, 3, 8, 9, 1, 3, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_968\\274467728.py:5: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2985.)\n",
      "  print(softmax_regression(x[:18]).max(dim=1).indices.T)\n"
     ]
    }
   ],
   "source": [
    "for x,y in test_data_loader:\n",
    "    labels = labels_int_to_text(y[:18])\n",
    "    y_hat = softmax_regression(x[:18])\n",
    "    print(y_hat)\n",
    "    print(softmax_regression(x[:18]).max(dim=1).indices.T)\n",
    "    # predictions = labels_int_to_text(softmax_regression(x[:18]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "print(111)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6a5381c313c4ee6411d11fe4393f2bbb58c85b55ae7c8c208703d8b6049241e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
